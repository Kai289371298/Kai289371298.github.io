\noindent\textbf{Research Question:}
The task of open-vocabulary image segmentation requires accurately identifying and segmenting visual entities in images based on given language descriptions that are \emph{open-ended and possibly not seen during training}. Unlike traditional image segmentation, which relies on a pre-defined set of object categories, open-vocabulary segmentation aims to generalize to a broader range of visual concepts, allowing for more flexible and adaptable image understanding in an open and realistic world. Typically, this task involves leveraging large-scale visual and linguistic data with extensive labels to create models capable of recognizing and segmenting novel visual inputs, but such labeled data may not be accessible due to the expensive human annotations. In this work, we aim to explore a \emph{self-supervised} approach, which builds an open-vocabulary segmentation model, requiring no supervision from humans.

\noindent\textbf{Technical Challenges:}
Previously in the computer vision literature, self-supervised learning approaches are mainly developed for image-level representation learning. Moving from image classification to open-vocabulary image segmentation, there are mainly two challenges: 1) \emph{Fine-grained self-supervision signals}: In this task, we need to acquire segmentation masks to train the segmentation model. To mimic human perception, the masks should cover semantically coherent regions, which represent objects and their constituent parts. 2) \emph{Language alignment:} The model needs to understand language-based, open-vocabulary prompts, and provide the correct segmentation masks. With the absence of human supervision, it would be challenging to accurately align the visual perception with language comprehension.

\noindent\textbf{Technical Approach and Novelty:}
We plan to investigate this problem by extending and combining existing pre-trained visual models including DINO and CLIP. DINO, as a self-supervised model, is able to produce well-localized visual features. By grouping patches with similar visual features, we identify semantically coherent regions which may represent meaningful visual entities. Meanwhile, the CLIP model, which is pre-trained to provide vision-language aligned features, is employed to generate guidance for semantic understanding. Combining the segmentation masks from DINO and language alignment from CLIP, we transfer the advantages of both pre-trained models into one unified open-vocabulary image segmentation model. In contrast to prior approaches that require dense human supervision, we fully rely on self-supervision to develop this segmentation model.

\noindent\textbf{Expected Outcomes and Preliminary Work:}
We expect to develop a self-supervised model capable of open-vocabulary image segmentation, which is potentially more scalable and generalizable since it is not dependent on costly human-created labels or biased by annotators. This model can serve as an off-the-shelf tool for visual understanding, or be utilized as starting point for further fine-tuning in specialized domains. Currently, we have developed a successful approach for self-supervised image segmentation based on DINO, achieving high segmentation performance comparable with supervised models. We will continue our exploration focusing on integrating this self-supervised segmentation method with CLIP-based vision-language comprehension, to enable open-vocabulary image segmentation.

\noindent\textbf{Resource Needs:} To accomplish our research goals, we require computation resources focused on GPU-centered model training. More specifically, our work will be supported by 1) \emph{Multi-GPU nodes}: Training image segmentation models stably would require a large enough batch size (typically 16 images/batch), so multi-GPU parallel training is necessary. Each GPU should be equipped with at least 32 GB memory. We require about \emph{20,000 GPU hours} to accomplish our research project. 2) \emph{Efficient and convenient I/O}: Our training pipeline loads and pre-processes the dataset in a unified manner, which needs to be shared across multiple computation nodes. We also need to save the learned parameters during model training to the shared storage. The total required storage for datasets and models is about \emph{10 TB}. 3) \emph{Software support}: The deep learning computation frameworks are based on CUDA. Other dependencies can be configured with Conda virtual environments. Through ACCESS, the \emph{NCSA Delta} system can satisfy our resource needs.
